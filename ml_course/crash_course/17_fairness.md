#Fairness:  
Before putting a model into production, it's critical to audit training data and evaluate predictions for bias. Human 
bias can manifest within the training data.

####Common Types of Bias:  
Any human involvement with the collection of data can introduce bias to the system, it is important to be aware of this 
when building a model so their effects can be mitigated.

* **Reporting Bias:** occurs when the frequency of events, properties or outcomes captured in the data doesnt reflect 
their real world frequency This bias can arise because people tend to focus on documenting circumstances that are 
unusual or especially memorable, assuming that the ordinary can "go without saying."

* **Automation Bias:** this is a tendency to favor results generated by automated systems over those generated by 
non-automated systems, irrespective of the error rates of each.

* **Selection Bias:** occurs if a data set's examples are chosen in a way that is not reflective of their real-world 
distribution. Selection bias can take many different forms:
    * **Coverage bias:** Data is not selected in a representative fashion.
    * **Non-response bias (participation bias)**: Data ends up being unrepresentative due to participation gaps in 
    the data-collection process.
    * **Sampling bias:** Proper randomization is not used during data collection.
    
* **Group Attribution Bias:** a tendency to generalize what is true of individuals to an entire group to which they 
belong. Two key manifestations of this bias are:
    * **In-group bias:** A preference for members of a group to which you also belong, or for characteristics that 
    you also share.
    * **Out-group homogeneity bias:** A tendency to stereotype individual members of a group to which you do not belong, 
    or to see their characteristics as more uniform
    
* **Implicit  Bias:** occurs when assumptions are made based on one's own mental models and personal experiences that 
do not necessarily apply more generally. A common form of implicit bias is confirmation bias, where model builders 
unconsciously process data in ways that affirm preexisting beliefs and hypotheses.

####Identifying Bias:  
A model should best represent the data that has been input into it, identifying and removing bias is a key part of this:

* **Missing Feature Values:** if your data set has one or more features that have missing values for a large number of 
examples, that could be an indicator that certain key characteristics of your data set are under-represented.

* **Unexpected Feature Values:** some feature values can be much greater or smaller than the expected value, this can
skew the model and indicate problems that occurred during data collection or other inaccuracies that could introduce 
bias.

* **Data Skew:**  certain groups or characteristics may be under or over-represented relative to their real-world 
prevalence. Failing to randomize data can cause this.

####Evaluating for Bias:  
Metrics calculated against an entire test or validation set don't always give an accurate picture of how fair the model 
is. Using precision and recall metrics across the whole model can give a general result, for example 80& precision and 
73% recall. However if this was split into say the result for male and female separately then one group could have a 
much greater precision/recall than the other, which was masked by doing the analysis over the whole model.

<img src="https://latex.codecogs.com/gif.latex?Precision=\frac{TP}{TP&plus;FP}" />  
<img src="https://latex.codecogs.com/gif.latex?Recall=\frac{TP}{TP&plus;FN}" />  

####fairness.py:
Example of evaluating a models fairness:
* Looking at how bias can manifest in a model
* Explore feature data to identify bias
* Evaluate model performance by sub groups rather than in aggregate
